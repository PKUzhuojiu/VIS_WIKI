{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取历史条目``edit_history.csv``之后，用本程序获取每次编辑后页面的内容\n",
    "\n",
    "这里爬取了每个页面源码中id为bodyContent的部分，结果保存在webcontent文件夹内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to get page, id: 3854, url_id: 997013342\n",
      "succeed to get page 3854!\n",
      "try to get page, id: 3855, url_id: 997012719\n",
      "succeed to get page 3855!\n",
      "try to get page, id: 3856, url_id: 997011238\n",
      "succeed to get page 3856!\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import csv\n",
    "import unicodedata\n",
    "import numpy\n",
    "import json\n",
    "\n",
    "# edit_history.csv保存了所有历史编辑页面的id，通过这个id可以获取到历史页面的链接，也即是urlhead+oldid\n",
    "# 每次爬取时，先确定要爬取的页面id范围，下面是edit_history.csv的第1到第2个id的示例代码\n",
    "# 需要修改的地方有：\n",
    "# 1. start_row, max_row 起止id的索引\n",
    "# 2. urlhead title修改成目标词条\n",
    "# 3. proxies修改成自己的代理\n",
    "\n",
    "\n",
    "urlhead = \"https://en.wikipedia.org/w/index.php?title=Donald_Trump&oldid=\" # 请修改\n",
    "\n",
    "s = rq.session()\n",
    "s.proxies = {\"https\": \"127.0.0.1:1080\"}  # 修改成自己的代理\n",
    "s.keep_alive = False\n",
    "\n",
    "max_row = 3844#7610 # 请修改\n",
    "start_row = 3834#3834 # 请修改\n",
    "with open(\"../settings.json\", \"r\") as f_set:\n",
    "    setting_json = f_set.read()\n",
    "    jstype = json.loads(setting_json)\n",
    "    start_row = jstype[\"startid\"]\n",
    "    max_row = jstype[\"endid\"]\n",
    "\n",
    "with open('edit_history.csv', 'r', encoding='utf-8') as f1:\n",
    "    rows = csv.reader(f1)\n",
    "    for row in rows:\n",
    "        id = int(row[0])\n",
    "        if id < start_row:\n",
    "            continue\n",
    "        if id > max_row:\n",
    "            print('finish')\n",
    "            break\n",
    "        page_oldid = str(row[1])\n",
    "        print(f'try to get page, id: {id}, url_id: {page_oldid}')\n",
    "        page_link = urlhead + page_oldid\n",
    "        try:\n",
    "            r = s.get(page_link, timeout=60)\n",
    "            r.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(r.text, 'lxml')\n",
    "            dateDOM = soup.find('span', id='mw-revision-date')\n",
    "            revdate = dateDOM.get_text()\n",
    "            with open(f'./content_{id}_{page_oldid}'+'.txt', 'w', encoding='utf-8') as f2:\n",
    "                body = soup.find('div', id='bodyContent')\n",
    "                comments = body.findAll(text=lambda text:isinstance(text, Comment))\n",
    "                [comment.extract() for comment in comments]\n",
    "                f2.write(f'{str(id)} {str(page_oldid)} {revdate}\\n')\n",
    "                f2.write(str(body))\n",
    "                print(f'succeed to get page {id}!')\n",
    "        except:\n",
    "            print(f'fail to get page {id}!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "407730e7b1f53fc8a0b1d9241751e40cdef0af9ebe90eb3a995d30f08872e3ca"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
