{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to get page, id: 3834, url_id: 997508760\n",
      "succeed to get page 3834!\n",
      "try to get page, id: 3835, url_id: 997427225\n",
      "succeed to get page 3835!\n",
      "try to get page, id: 3836, url_id: 997402055\n",
      "succeed to get page 3836!\n",
      "try to get page, id: 3837, url_id: 997326056\n",
      "succeed to get page 3837!\n",
      "try to get page, id: 3838, url_id: 997325170\n",
      "succeed to get page 3838!\n",
      "try to get page, id: 3839, url_id: 997324848\n",
      "succeed to get page 3839!\n",
      "try to get page, id: 3840, url_id: 997277580\n",
      "succeed to get page 3840!\n",
      "try to get page, id: 3841, url_id: 997198306\n",
      "succeed to get page 3841!\n",
      "try to get page, id: 3842, url_id: 997138732\n",
      "succeed to get page 3842!\n",
      "try to get page, id: 3843, url_id: 997136735\n",
      "succeed to get page 3843!\n",
      "try to get page, id: 3844, url_id: 997136138\n",
      "succeed to get page 3844!\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import csv\n",
    "import unicodedata\n",
    "import numpy\n",
    "\n",
    "# 爬取的html信息中仍有冗余，本程序用于删除其中不关心的信息\n",
    "# 从webcontent文件中的txt文件读入数据，处理后写入processedContent文件夹\n",
    "# 可以写入为html或者txt\n",
    "# html页面可以直接双击打开，便于查看\n",
    "# 写入txt前，可以进一步删除style等信息，以便匹配\n",
    "\n",
    "# 需要修改的地方有：\n",
    "# 1. start_row, max_row 起止id的索引\n",
    "\n",
    "max_row = 3844 # 请修改\n",
    "start_row = 3834 # 请修改\n",
    "\n",
    "with open('edit_history.csv', 'r', encoding='utf-8') as f1:\n",
    "    rows = csv.reader(f1)\n",
    "    for row in rows:\n",
    "        id = int(row[0])\n",
    "        if id < start_row:\n",
    "            continue\n",
    "        if id > max_row:\n",
    "            print('finish')\n",
    "            break\n",
    "        page_oldid = str(row[1])\n",
    "        print(f'try to get page, id: {id}, url_id: {page_oldid}')\n",
    "        try:          \n",
    "            with open(f'./webcontent/content_{id}_{page_oldid}'+'.txt', 'r', encoding='utf-8') as f2:\n",
    "                r = f2.read()\n",
    "                soup = BeautifulSoup(r, 'lxml')\n",
    "                \n",
    "                # 删除掉一些内容\n",
    "                extractList = []\n",
    "                # noprint\n",
    "                noprints = soup.findAll('div', class_='noprint')\n",
    "                extractList.extend(noprints)\n",
    "                # [noprint.extract() for noprint in noprints]\n",
    "                # contentSub\n",
    "                contentsub = soup.find('div', id='contentSub')\n",
    "                # contentsub.extract()\n",
    "                extractList.append(contentsub)\n",
    "                # contentsub2\n",
    "                contentsub2 = soup.find('div', id='contentSub2')\n",
    "                # contentsub2.extract()\n",
    "                extractList.append(contentsub2)\n",
    "                # jump link\n",
    "                jump_link = soup.findAll('a', class_='mw-jump-link')\n",
    "                extractList.extend(jump_link)\n",
    "                # hatnote\n",
    "                hatnotes = soup.findAll('div', class_='hatnote')\n",
    "                extractList.extend(hatnotes)\n",
    "\n",
    "                [i.extract() for i in extractList]\n",
    "                # write in html\n",
    "                with open(f'./processedContent/content_{id}_{page_oldid}'+'.html', 'w', encoding='utf-8') as f3:\n",
    "                    f3.write(str(soup))\n",
    "                \n",
    "                extractList = []\n",
    "                \n",
    "                [i.extract() for i in extractList]\n",
    "                # write in txt\n",
    "                with open(f'./processedContent/{id}'+'.txt', 'w', encoding='utf-8') as f4:#content_{id}_{page_oldid}'+'.txt', 'w', encoding='utf-8') as f4:\n",
    "                    f4.write(str(soup))\n",
    "                print(f'succeed to get page {id}!')\n",
    "        except:\n",
    "            print(f'fail to get page {id}!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "407730e7b1f53fc8a0b1d9241751e40cdef0af9ebe90eb3a995d30f08872e3ca"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
